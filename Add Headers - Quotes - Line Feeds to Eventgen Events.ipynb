{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bcde19e",
   "metadata": {},
   "source": [
    "# Adding Headers-Quotes-Line Feeds to Eventgen Events\n",
    "\n",
    "James H Baxter  \n",
    "Jan 17 2022  \n",
    "\n",
    "## Introduction\n",
    "\n",
    "This code was developed to modify events which were originally ingested via the Splunk Add-on for Linux and Unix and have been exported from a Splunk search so that they can be used with Splunk Eventgen to create artifical events of this type.  \n",
    "\n",
    "When you export sample Linux/Unix OS-related events (cpu, disk, interfaces, etc.) from Splunk, you get events with just the data fields. What isn't obvious at first is that when these events were generated & sent to Splunk indexers by a forwarder, they originally included a header (for each event) and extra line feeds. When you see the events in a Splunk search, they only include the data fields - no header (vstat being the only exception I'm aware of) - but the fields have been properly parsed and identified (CPU & such) and appear in the left-hand 'Interesting Fields' list in Splunk (using Smart or Verbose Mode). An initial investigation of the props/transforms in the Add-on doesn't make it obvious where/how these fields were extracted or identified - but there are field aliases and evals for creating additional fields. It's only when you look at the scripts in /bin (cpu.sh, for example) that you see that each event was created with a smaller number of specific fields, and included a header. In many cases, there is also an extra line feed added. Doing a comparison of the various .sh files in /bin and another look through the props file and this all starts to make sense.  \n",
    "\n",
    "So - after exporting some sample Linux/Unix events from Splunk, you have to edit those events to add the header and extra line feeds - recreating the events in the format they were in when sent to Splunk for indexing - before you can use them with Eventgen to create artifical events. Otherwise, the props/transforms from the Splunk Add-on for Linux/Unix will not properly parse the events.  \n",
    "\n",
    "This code adds the headers and extra line feeds to events from a provided sample file, and creates a new output sample file (so that the origin file is not modified). It also adds quotes around the events, which may nor may not be needed but doesn't seem to hurt.\n",
    "\n",
    "The other problem with these Linux/Unix events is that their timestamps (as they appear in Splunk searches) were created at index-time - the events themselves did not include a timestamp. When you export these events for use with Eventgen, they again do not include a timestamp. There are two ways of dealing with this situation that I'm aware of:\n",
    "\n",
    "1. Use the mode = sample option in eventgen.con, which blasts all of the events in your sample file out at once; Splunk will give them an index-time timestamp - but they be all have the same timestamp per generation interval. Example:  \n",
    "\n",
    "```text\n",
    "[linux_os_events_10000.csv]\n",
    "disabled = 0\n",
    "mode = sample\n",
    "timeField = _time\n",
    "sampletype = csv\n",
    "interval = 60\n",
    "earliest = -60s\n",
    "latest = now\n",
    "```\n",
    "\n",
    "2. My preference is to use mode = replay and the timeField = ```_time``` option, which requires you to include the ```_time``` field in the Splunk export:\n",
    "\n",
    "```text\n",
    "[linux_os_events_10000.csv]\n",
    "disabled = 0\n",
    "mode = replay\n",
    "timeField = _time\n",
    "sampletype = csv\n",
    "interval = 60\n",
    "earliest = -60s\n",
    "latest = now\n",
    "```\n",
    "\n",
    "The search for exporting these events from Splunk is (substituting the correct index(es) for your environment):  \n",
    "\n",
    "```text\n",
    "index=linux_os\n",
    "| reverse\n",
    "| table index,host,source,sourcetype,_time,_raw\n",
    "\n",
    "Exclude the _time column if you are not going to use this approach. Instead, use:\n",
    "\n",
    "index=linux_os\n",
    "| reverse\n",
    "| table index,host,source,sourcetype,_raw\n",
    "\n",
    "and remove the timeField = _time entry from the stanza.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ab607ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "inFile =  \"linux_unix_addon_events_10000.csv\"\n",
    "outFile = \"linux_unix_addon_events_10000_formatted.csv\"\n",
    "\n",
    "# The first (commented out) df is from version 8.4.0 of the add-on; the second is from version 8.3.1\n",
    "# The 'Siize' field in the v8.3.1 df header is what is in the df.sh script\n",
    "\n",
    "# vmstat is commented out in the list below because those events already have headers in the Splunk export.\n",
    "# Note that the spacing between the various header fields must be maintained exactly for parsing to work.\n",
    "\n",
    "headers = {\n",
    "\"bandwidth\":\"Name    rxPackets_PS txPackets_PS rxKB_PS txKB_PS\",\n",
    "\"cpu\":\"CPU    pctUser    pctNice  pctSystem  pctIowait    pctIdle\",\n",
    "\"cpu_metric\":\"CPU    pctUser    pctNice  pctSystem  pctIowait    pctIdle    OSName                                   OS_version  IP_address\",\n",
    "#\"df\":\"Filesystem\\tType\\tSize\\tUsed\\tAvail\\tUsePct\\tINodes\\tIUsed\\tIFree\\tIUsePct\\tMountedOn\",\n",
    "\"df\":\"Filesystem                                          Type              Siize        Used       Avail      UsePct    MountedOn\",\n",
    "\"df_metric\":\"Filesystem\\tType\\tSize\\tUsed\\tAvail\\tUsePct\\tINodes\\tIUsed\\tIFree\\tIUsePct\\tOSName\\tOS_version\\tIP_address\\tMountedOn\",\n",
    "\"interfaces\":\"Name       MAC                inetAddr         inet6Addr                                  Collisions  RXbytes          RXerrors         TXbytes          TXerrors         Speed        Duplex\",\n",
    "\"interfaces_metric\":\"Name       MAC                inetAddr         inet6Addr                                  Collisions  RXbytes          RXerrors         TXbytes          TXerrors         Speed        Duplex          OSName                                   OS_version  IP_address\",\n",
    "\"iostat\":\"Device          rReq_PS      wReq_PS        rKB_PS        wKB_PS  avgWaitMillis   avgSvcMillis   bandwUtilPct\",\n",
    "\"iostat_metric\":\"Device          rReq_PS      wReq_PS        rKB_PS        wKB_PS  avgWaitMillis   avgSvcMillis   bandwUtilPct    OSName                                   OS_version  IP_address\",\n",
    "\"lastlog\":\"USERNAME                        FROM                            LATEST\",\n",
    "\"lsof\":\"COMMAND     PID        USER   FD      TYPE             DEVICE     SIZE       NODE NAME\",\n",
    "\"netstat\":\"Proto  Recv-Q  Send-Q  LocalAddress                    ForeignAddress                  State\",\n",
    "\"nfsiostat\":\"Mount \t\t  Path\t\t  r_op/s    w_op/s    r_KB/s    w_KB/s    rpc_backlog    r_avg_RTT    w_avg_RTT    r_avg_exe    w_avg_exe\",\n",
    "\"openPorts\":\"Proto   Port\",\n",
    "\"package\":\"NAME                                                     VERSION               RELEASE               ARCH        VENDOR                          GROUP\",\n",
    "\"protocol\":\"  IPdropped   TCPrexmits   TCPreorder   TCPpktRecv   TCPpktSent   UDPpktLost   UDPunkPort   UDPpktRecv   UDPpktSent\",\n",
    "\"ps\":\"USER               PID   PSR   pctCPU       CPUTIME  pctMEM     RSZ_KB     VSZ_KB   TTY      S       ELAPSED  COMMAND             ARGS\",\n",
    "\"ps_metric\":\"USER                                   PID   PSR   pctCPU       CPUTIME  pctMEM     RSZ_KB     VSZ_KB   TTY      S          ELAPSED    OSName                                   OS_version  IP_address        COMMAND             ARGS\",\n",
    "\"top\":\"   PID  USER              PR    NI    VIRT     RES     SHR   S  pctCPU  pctMEM       cpuTIME  COMMAND\",\n",
    "\"usersWithLoginPrivs\":\"USERNAME                      UID                             GID                             HOME_DIR                                                      USER_INFO\",\n",
    "#\"vmstat\":\"memTotalMB   memFreeMB   memUsedMB  memFreePct  memUsedPct   pgPageOut  swapUsedPct   pgSwapOut   cSwitches  interrupts       forks   processes     threads  loadAvg1mi  waitThreads    interrupts_PS    pgPageIn_PS    pgPageOut_PS\",\n",
    "#\"vmstat_metric\":\"memTotalMB   memFreeMB   memUsedMB  memFreePct  memUsedPct   pgPageOut  swapUsedPct   pgSwapOut   cSwitches  interrupts       forks   processes     threads  loadAvg1mi  waitThreads    interrupts_PS    pgPageIn_PS    pgPageOut_PS    OSName                                   OS_version  IP_address\",\n",
    "\"who\":\"USERNAME        LINE        HOSTNAME                                  TIME\"\n",
    "}\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "fo = open(outFile, \"w\")\n",
    "\n",
    "def add_header_quotes_linefeeds(sourcetype, data):\n",
    "    \n",
    "    # if the data already has qutoes, strip off the quotes first\n",
    "    if data.startswith('\"') and data.endswith('\"'):\n",
    "        data = data[1:-1]\n",
    "    else:\n",
    "        # add header and a linefeed\n",
    "        if sourcetype in headers:\n",
    "            data = headers[sourcetype] + \"\\n\" + data\n",
    "        \n",
    "        # wrap data in qutoes\n",
    "        data = '\"' + data + '\"'\n",
    "    \n",
    "        # add two linefeeds\n",
    "        data = data + \"\\n\\n\"\n",
    "    \n",
    "        return data\n",
    "\n",
    "\n",
    "with open(inFile) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        sourcetype = row[3]\n",
    "        \n",
    "        # Print the sample file header row as-is\n",
    "        # index,host,source,sourcetype,_time,_raw\n",
    "        if line_count == 0:\n",
    "            lineout = \",\".join(row)\n",
    "            lineout = lineout + \"\\n\"\n",
    "            fo.write(lineout)\n",
    "            line_count += 1\n",
    "            continue\n",
    "\n",
    "        # Strip the milliseconds & TZ portion of the timestamp off, if present\n",
    "        # since Eventgen doesn't seem to process those corretly even with good regex,\n",
    "        # and timestamps from this source don't appear to use milliseconds anyway\n",
    "        # 2017-08-29T01:12:50.000+0000  to   2017-08-29T01:12:50\n",
    "        \n",
    "        # if using a _time column in the source file, _raw is row[5]\n",
    "        if len(row) == 6:\n",
    "            \n",
    "            # in this case, row[4] is _time column\n",
    "            timestamp = row[4]\n",
    "            if '.' in timestamp:\n",
    "                idx = timestamp.index('.')\n",
    "                timestamp = timestamp[:idx]\n",
    "            row[4] = timestamp\n",
    "            rawdata = row[5]\n",
    "\n",
    "        # len(row) != 6 so assuming index,host,source,sourcetype,_raw\n",
    "        # and _raw is in row[4]\n",
    "        else:\n",
    "            rawdata = row[4]\n",
    "\n",
    "        # process this row's _raw data\n",
    "        row[5] = add_header_quotes_linefeeds(sourcetype, rawdata)\n",
    "        lineout = \",\".join(row)\n",
    "        if DEBUG: print(lineout)\n",
    "        fo.write(lineout)\n",
    "        line_count += 1\n",
    "            \n",
    "    if DEBUG: print(f'Processed {line_count} lines.')\n",
    "    \n",
    "fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759204ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
